{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формальные языки\n",
    "\n",
    "**Формальный язык** — множество конечных слов над конечным алфавитом $\\Sigma$. \n",
    "Пусть есть некоторое конечно множество символов $\\Sigma$, тогда множество $L \\in \\Sigma^*$ есть формальный язык. \n",
    "\n",
    "Над формальными языками можно определить операции:\n",
    "\n",
    "* $L_1 \\cap L_2$\n",
    "* $L_1 \\cup L_2$\n",
    "* $L_1 \\setminus L_2$\n",
    "* $L_1 \\cdot L_2 $ - новый язык, в котором ко всем возможным словам из $L_1$ присоеденены справа слова из $L_2$\n",
    "* $L^*$ - замыкание клини, $\\{\\epsilon\\} \\cup L \\cup (L \\cdot L) \\cup (L \\cdot L \\cdot L) \\cup \\cdots$\n",
    "\n",
    "**Иерархия Хомского** — классификация формальных языков и формальных грамматик, согласно которой они делятся на 4 типа по их условной сложности"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Контекстно-свободная грамматикой**  называется четверка $G = (V, \\Sigma, R, S)$, где\n",
    "\n",
    "- $V$  - конечное множество нетерминальных символов\n",
    "- $\\Sigma$ - конечно множество терминальных символов (алфавит формального языка)\n",
    "- $R$  - конечное множество правил  вида $V \\rightarrow (V \\cup \\Sigma)^{*}$\n",
    "- $S \\in V$ - начальный нетерминал\n",
    "\n",
    "### Примеры КС-грамматик\n",
    "\n",
    "**Язык правильных скобочных записей**\n",
    "\n",
    "$S \\rightarrow (S)S$\n",
    "\n",
    "$S \\rightarrow \\epsilon$\n",
    "\n",
    "Нетерминалы: $\\{S\\}$, терминалы: $\\{(, )\\}$, начальный нетерминал: $S$\n",
    "\n",
    "**Математические выражения**\n",
    "\n",
    "$S \\rightarrow S + P\\,|\\,S - P\\,|\\,P$\n",
    "\n",
    "$P \\rightarrow A\\,|\\,P \\cdot A\\,|\\,P / A$\n",
    "\n",
    "$A \\rightarrow  \\mathbb{num}|\\,(S)$\n",
    "\n",
    "Нетерминалы: $\\{S, P, A\\}$, терминалы: $\\{+, -, \\dot, (, ), \\mathbb{num}\\}$, начальный нетерминал: $S$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбор КС-грамматик\n",
    "\n",
    "Задача алгоритма разбора: по заданной грамматике и строке определить, принадлежит ли строка языку, порождаемому этой грамматикой, и построить дерево разбора\n",
    "\n",
    "Существуют различные типы КС-грамматик:\n",
    "- $LL$ \n",
    "- $LALR$\n",
    "- $SLR$\n",
    "- $LR$\n",
    "\n",
    "Распознаватели строятся по алгоритмам оптимальным для соответствующего типа (применяются при разборе языков программирования). Разбор входной строки обычно идет слева направо, дерево вывода может строиться сверху вниз или снизу вверх.\n",
    "\n",
    "Любая КС-грамматика может быть преобразована к эквивалентной грамматике к нормальной форме Хомского. Грамматика имеет вид нормальной формы Хомского, если ее правила имеют вид:\n",
    "\n",
    "$\\: A \\rightarrow BC$ \n",
    "\n",
    "$\\: A \\rightarrow \\alpha$ \n",
    "\n",
    "$\\: S \\rightarrow \\epsilon$\n",
    "\n",
    "**Алгоритм Кока — Янгера — Касами (CYK)** - алгоритм синтаксического анализа статьи, реализует вывод снизу-вверх, используется динамическое программирование. Сложность - $\\mathcal{O}\\left( n^3 \\cdot \\left| R \\right| \\right)$, где $n$ - размер строки, $R$ - правила грамматики в нормально форме Хомского.\n",
    "\n",
    "Идея: пусть входная строк $w$. Строится таблица $d[A][i][j]$ с данными о возможности вывода  $w[i..j]$ из правила $A$.\n",
    "\n",
    "**Алгоритм Эрли** - динамический алгоритм, строит вывод сверху вниз. Не требует преобразования к нормальной форме Хомского."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Иерархия Хомского\n",
    "\n",
    " Классификация формальных грамматик (и, соответственно, порождаемых ими языков), согласно которой они делятся на 4 типа, в зависимости от их  сложности. Предложена лингвистом Ноамом Хомским. \n",
    "\n",
    " #### Тип 3 — регулярные\n",
    " Сюда входят регулярные языки.  Регулярный язык (помимо регулярных выражений, конечных автоматов) можно задать с помощью КС-грамматик, где продукции выглядят так: $A \\rightarrow a$ или $A \\rightarrow aB$\n",
    "\n",
    "#### Тип 2 — контекстно-свободные\n",
    "Контекстно свободные грамматики и порождаемые ими языки\n",
    "\n",
    "#### Тип 1 — контекстно-зависимые\n",
    "Контекстно зависимые грамматики и порождаемые ими языки. Правила выводы выглядят так $\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$\n",
    "\n",
    "#### Тип 0 — неограниченные\n",
    "Нет никаких ограничений"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотека [LARK](https://github.com/lark-parser/)\n",
    "\n",
    "Библиотека синтаксического разбора для `Python`. Реализует алгоритм Эрли и $LALR(1)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lark import Lark, Tree\n",
    "\n",
    "\n",
    "calc_grammar = \"\"\"\n",
    "    ?start: sum\n",
    "\n",
    "    ?sum: product\n",
    "        | sum \"+\" product   -> add\n",
    "        | sum \"-\" product   -> sub\n",
    "\n",
    "    ?product: atom\n",
    "        | product \"*\" atom  -> mul\n",
    "        | product \"/\" atom  -> div\n",
    "\n",
    "    ?atom: NUMBER           -> number\n",
    "         | \"(\" sum \")\"\n",
    "\n",
    "    %import common.CNAME -> NAME\n",
    "    %import common.NUMBER\n",
    "    %import common.WS_INLINE\n",
    "\n",
    "    %ignore WS_INLINE\n",
    "\"\"\"\n",
    "\n",
    "calc_parser = Lark(calc_grammar, parser='lalr')\n",
    "tree = calc_parser.parse(\"(1 + 2) * 5 + 123\")\n",
    "\n",
    "def calc(tree: Tree) -> int:\n",
    "    match tree.data:\n",
    "        case \"mul\":\n",
    "            return calc(tree.children[0]) * calc(tree.children[1])\n",
    "        case \"sub\":\n",
    "            return calc(tree.children[0]) - calc(tree.children[1])\n",
    "        case \"add\":\n",
    "            return calc(tree.children[0]) + calc(tree.children[1])\n",
    "        case \"div\":\n",
    "            return calc(tree.children[0]) / calc(tree.children[1])\n",
    "        case \"number\":\n",
    "            return int(tree.children[0])\n",
    "\n",
    "calc(tree)\n",
    "##print(tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Pyparsing](https://github.com/pyparsing/pyparsing/)\n",
    "\n",
    "Ещё одна библиотека синтаксического разбора для `Python`. Грамматика описывается с помощью специального DSL (domain-specific language, предметно-ориентированный язык).\n",
    "\n",
    "Опишем грамматику, котора позволяет разобрать записи вида: \n",
    ">   <слово>: число, число, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParseResults(['hello', '1', '22', '3'], {})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyparsing import Word, alphas, nums,  Literal, StringEnd, ZeroOrMore, Suppress, OneOrMore \n",
    "\n",
    "word = Word(alphas)\n",
    "num = Word(nums)\n",
    "sep = Suppress(OneOrMore(','))\n",
    "col = Suppress(':')\n",
    "\n",
    "s = word + col + num + ZeroOrMore(sep + num) + StringEnd()\n",
    "        \n",
    "s.parseString('hello: 1, 22, 3')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более сложный пример, грамматика описывает правильные скобочные записи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParseResults(['(', '(', ')', ')', '(', ')', '(', ')'], {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyparsing import Literal, Forward, StringEnd, OneOrMore, Empty\n",
    "\n",
    "br_o = Literal('(')\n",
    "br_c = Literal(')')\n",
    "\n",
    "braces = Forward()\n",
    "braces << OneOrMore(br_o + (braces | Empty() ) + br_c)\n",
    "start = braces + StringEnd()\n",
    "        \n",
    "start.parseString('(())()()')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [YARGY](https://github.com/natasha/yargy)\n",
    "\n",
    "Библиотека для извлечения структурированных данных из текста на русском языке. Аналог [Tomita Parser](https://github.com/yandex/tomita-parser/).\n",
    "\n",
    "Для разбора текста используется алгоритм Эрли и Pymorphy2 для работы с морфологией."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем простую грамматику для поиска упоминаний улиц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['статьи', '123']\n"
     ]
    }
   ],
   "source": [
    "from yargy import Parser, rule, and_, or_\n",
    "from yargy.predicates import gte, lte, eq, normalized\n",
    "# from yargy.pipelines import morph_pipeline\n",
    "\n",
    "text = \"Аппеляционная эалоба рассмотрена в порядке статьи 123, 156 Арбиртражного Процессуального Кодекса РФ\"\n",
    "\n",
    "gr = rule(\n",
    "    or_(rule(\"ст\", eq(\".\").optional()),\n",
    "        rule(normalized(\"статья\"))),\n",
    "    and_(gte(1), lte(1000))\n",
    ")\n",
    "\n",
    "for match in Parser(gr).findall(text):\n",
    "    print([t.value for t in match.tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Таврической', 'улице']\n",
      "['Первой', 'Большой', 'Рождественской', 'улицы']\n",
      "['Мариинский', 'проезд']\n"
     ]
    }
   ],
   "source": [
    "from yargy import Parser, rule, and_, or_\n",
    "from yargy.predicates import gram, is_capitalized, dictionary\n",
    "\n",
    "\n",
    "ST = rule(\n",
    "    and_(\n",
    "        gram(\"ADJF\"),  # прилагательное\n",
    "        is_capitalized() # с большой буквы\n",
    "    ),\n",
    "    gram(\"ADJF\").repeatable().optional(), # дальше может быть прилагательное (optional)\n",
    "    dictionary({\n",
    "        \"улица\",\n",
    "        \"переулок\",\n",
    "        \"проезд\"\n",
    "    })  # аналог morph_pipeline, когда хотим найти что-то, что в нормализованном виде попадает в dictionary\n",
    ")\n",
    "\n",
    "\n",
    "text = \"Я шел по городу от Таврической улице и в итоге дошел до Первой Большой Рождественской улицы. Мариинский проезд остался позади.\"  \n",
    "parser = Parser(ST)\n",
    "for match in parser.findall(text):\n",
    "    print([_.value for _ in match.tokens])\n",
    "    # display(match.tree.as_dot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более сложные грамматики для определения адресов и других именованных сущностей есть в библиотеке [Natasha](https://github.com/natasha/natasha/blob/master/natasha/grammars/addr.py)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате работы парсера мы получаем дерево разбора. Иногда удобнее сразу интерпретировать узлы дерева в качестве объектов - фактов. Например, мы хотим извлечь данные о занимаемой должности и имени. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person(position='руководитель', name=Name(first='антон', last='иванов'))\n"
     ]
    }
   ],
   "source": [
    "from yargy import Parser\n",
    "from yargy.predicates import gram\n",
    "from yargy.pipelines import morph_pipeline\n",
    "from yargy.interpretation import fact\n",
    "from yargy.relations import gnc_relation\n",
    "\n",
    "Person = fact(\n",
    "    \"Person\",\n",
    "    [\"position\", \"name\"]\n",
    ")\n",
    "Name = fact(\n",
    "    \"Name\",\n",
    "    [\"first\", \"last\"]\n",
    ")\n",
    "\n",
    "gnc = gnc_relation() # грамматическое согласование\n",
    "\n",
    "POSITION = morph_pipeline([\n",
    "    \"директор\",\n",
    "    \"руководитель\"\n",
    "])\n",
    "\n",
    "ORG = morph_pipeline([\n",
    "    \"фирма\",\n",
    "    \"предприятие\"\n",
    "])\n",
    "\n",
    "NAME = rule(\n",
    "    gram(\"Name\").interpretation(\n",
    "        Name.first.inflected()\n",
    "    ),\n",
    "    gram(\"Surn\").interpretation(\n",
    "        Name.last.inflected()\n",
    "    )\n",
    ").interpretation(\n",
    "    Name\n",
    ")\n",
    "\n",
    "PERSON = rule(\n",
    "    POSITION.interpretation(\n",
    "        Person.position.inflected()\n",
    "    ),\n",
    "    ORG,\n",
    "    NAME.interpretation(\n",
    "        Person.name\n",
    "    )\n",
    ").interpretation(\n",
    "    Person\n",
    ")\n",
    "\n",
    "\n",
    "parser = Parser(PERSON)\n",
    "text = \"Руководителя предприятия Антона Иванова поздравил коллектив с праздником.\"\n",
    "for match in parser.findall(text):\n",
    "    print(match.fact)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда в правилах нужно иметь согласование по роду, числу или падежу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Петру', 'Марков']\n",
      "['Петр', 'Марков']\n"
     ]
    }
   ],
   "source": [
    "from yargy.relations import gnc_relation\n",
    "\n",
    "NAME = rule(\n",
    "    gram(\"Name\"),\n",
    "    gram(\"Surn\")\n",
    ")\n",
    "\n",
    "parser = Parser(NAME)\n",
    "for match in parser.findall(\"Петру Марков, Петр Марков\"):\n",
    "    print([_.value for _ in match.tokens])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в этом случае можно использовать `gnc_relation()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Петр', 'Марков']\n",
      "['У', 'Маркова']\n"
     ]
    }
   ],
   "source": [
    "gnc = gnc_relation()\n",
    "\n",
    "NAME = rule(\n",
    "    gram(\"Name\").match(gnc),\n",
    "    gram(\"Surn\").match(gnc)\n",
    ")\n",
    "\n",
    "\n",
    "parser = Parser(NAME)\n",
    "for match in parser.findall(\"Петру Марков, Петр Марков, У Маркова\"):\n",
    "    print([_.value for _ in match.tokens])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем простую грамматику, для извлечения данных вида: человек $\\rightarrow$ окончил такой-то университет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry(person=Person(first=None, last='иванов'), edu_org='университет Манчестера')\n"
     ]
    }
   ],
   "source": [
    "Person = fact(\n",
    "    \"Person\",\n",
    "    [\"first\", \"last\"]\n",
    ")\n",
    "\n",
    "EduOrg = fact(\n",
    "    \"EduOrg\",\n",
    "    [\"name\"]\n",
    ")\n",
    "\n",
    "Entry = fact(\n",
    "    \"Entry\",\n",
    "    [\"person\", \"edu_org\"]\n",
    ")\n",
    "\n",
    "gnc = gnc_relation()\n",
    "\n",
    "NAME = rule(\n",
    "    is_capitalized().match(gnc).interpretation(Person.first.inflected()).optional(),\n",
    "    is_capitalized().match(gnc).interpretation(Person.last.inflected()),\n",
    ").interpretation(\n",
    "    Person\n",
    ")\n",
    "\n",
    "GRAD_VERB = morph_pipeline([\n",
    "    \"окончил\",\n",
    "    \"закончил\",\n",
    "    \"выпустился из\", \n",
    "])\n",
    "\n",
    "EDU_ORG_TAG = morph_pipeline([\n",
    "    \"университет\", \n",
    "    \"институт\",\n",
    "    \"училище\",\n",
    "    \"техникум\",\n",
    "    \"военный институт\"\n",
    "])\n",
    "\n",
    "EDU_ORG_FULL_NAME = rule(\n",
    "    gram(\"ADJF\"),\n",
    "    gram(\"ADJF\").optional().repeatable(),\n",
    "    EDU_ORG_TAG\n",
    ")\n",
    "\n",
    "EDU_ORG_ABBR = rule(\n",
    "    gram(\"Abbr\")\n",
    ")\n",
    "\n",
    "EDU_ORG_CITY = rule(\n",
    "    EDU_ORG_TAG,\n",
    "    gram(\"NOUN\").optional().repeatable(),\n",
    "    and_(\n",
    "        is_capitalized(),\n",
    "        gram(\"NOUN\")\n",
    "    )\n",
    ")\n",
    "\n",
    "EDU_ORG = rule(\n",
    "    or_(\n",
    "        EDU_ORG_FULL_NAME,\n",
    "        EDU_ORG_ABBR,\n",
    "        EDU_ORG_CITY\n",
    "    )\n",
    ").interpretation(EduOrg.name)   \n",
    "\n",
    "S = rule(\n",
    "    NAME.interpretation(Entry.person),\n",
    "    GRAD_VERB,\n",
    "    EDU_ORG.interpretation(Entry.edu_org)\n",
    ").interpretation(Entry)\n",
    "\n",
    "text = \"Иванов окончил университет Манчестера\"\n",
    "\n",
    "parser = Parser(S)\n",
    "for match in parser.findall(text):\n",
    "    print(match.fact)\n",
    "    #print([_.value for _ in match.tokens])\n",
    "    #display(match.tree.as_dot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим на коллекции новостных документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterator\n",
    "\n",
    "@dataclass\n",
    "class Text:\n",
    "    label: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "\n",
    "def read_texts(fn: str) -> Iterator[Text]:\n",
    "    with gzip.open(fn, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield Text(*line.strip().split(\"\\t\"))\n",
    "\n",
    "texts = list(read_texts(\"../data/news.txt.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry(person=Person(first='дмитрий', last='циликин'), edu_org='Ленинградский государственный институт')\n",
      "Entry(person=Person(first=None, last='янина'), edu_org='Высшее театральное училище')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f78b518f550>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ffrankusha/study/university/nlp-course/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f78b518f550>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ffrankusha/study/university/nlp-course/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry(person=Person(first=None, last='самойлов'), edu_org='театральное училище')\n",
      "Entry(person=Person(first=None, last='людомирский'), edu_org='МГТУ')\n",
      "Entry(person=Person(first=None, last='шевченко'), edu_org='Луганское художественное училище')\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "for text in tqdm(texts, disable=True):\n",
    "    try:\n",
    "        for match in parser.findall(text.text):\n",
    "         print(match.fact)\n",
    "    except:\n",
    "       # empty\n",
    "       pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
